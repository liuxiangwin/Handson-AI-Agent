{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798dc68d-f7c5-481f-a72b-caa50fceb667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index --quiet\n",
    "!pip install llama-index-llms-huggingface --quiet\n",
    "!pip install llama-index-embeddings-huggingface --quiet\n",
    "!pip install llama-index-llms-langchain --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16e8e74c-7705-4cee-a48d-25e85af8b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "os.environ['ATHINA_API_KEY'] = \"IhrJrr0krTMRA9ogqi5aaD4ZuYuvMcdG\"\n",
    "\n",
    "\n",
    "INFERENCE_SERVER_URL = \"http://localhost:8989\"\n",
    "# MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "API_KEY= \"alanliuxiang\"\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_base= f\"{INFERENCE_SERVER_URL}/v1\",\n",
    "    model_name=MODEL_NAME,\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c79dbd-a646-4c86-9df4-80111221ea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Ignoring wrong pointing object 110 0 (offset 0)\n",
      "Ignoring wrong pointing object 127 0 (offset 0)\n",
      "Ignoring wrong pointing object 136 0 (offset 0)\n",
      "Ignoring wrong pointing object 138 0 (offset 0)\n",
      "Ignoring wrong pointing object 140 0 (offset 0)\n",
      "Ignoring wrong pointing object 142 0 (offset 0)\n",
      "Ignoring wrong pointing object 162 0 (offset 0)\n",
      "Ignoring wrong pointing object 164 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "# 加载电商财报数据\n",
    "# from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# A_docs = SimpleDirectoryReader(\n",
    "#     # input_files=[\"08-llamaindex-RAG-agent\\电商B-Third Quarter 2023 Results.pdf\"]\n",
    "#     input_files=[\"./电商A-Third Quarter 2023 Results.pdf\"]\n",
    "#     # loader = PyPDFLoader(\"./data/tesla_q3.pdf\")\n",
    "# ).load_data()\n",
    "# B_docs = SimpleDirectoryReader(\n",
    "#     # input_files=[\"08-llamaindex-RAG-agent\\电商B-Third Quarter 2023 Results.pdf\"]\n",
    "#     input_files=[\"./电商B-Third Quarter 2023 Results.pdf\"]\n",
    "# ).load_data()\n",
    "\n",
    "\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "A_docs = SimpleDirectoryReader(\"./companyA\").load_data()\n",
    "B_docs = SimpleDirectoryReader(\"./companyB\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99156079-1056-48b7-b210-5a7ff713babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pdf\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# loader_A = PyPDFLoader(\"./电商A-Third Quarter 2023 Results.pdf\")\n",
    "# A_docs = loader_A.load()\n",
    "\n",
    "# loader_B = PyPDFLoader(\"./电商A-Third Quarter 2023 Results.pdf\")\n",
    "# B_docs = loader_B.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba0bc933-046d-4058-b01d-efe28693c5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98dd5c11afd24209b44d1ee3664f9eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4bcdb8b741493faeca012880fa7db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015123a0faf7405296a8c5d093516cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/90.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a6b565dbef45879c7e363d566dca44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce90e779f0346d0a3e1850f819bd8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e825efd31814454b673a48237e16566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7758550430740e0b1c5a48d444ef878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ba00c1d22440298847f917232b689f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce5be7d639b435db9b6aa81408a239f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e85febb15c74527aae044f0fa1dc31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acefc5005583447eaab5d9e22b0786a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "Settings.llm = model\n",
    "Settings.embed_model = embed_model\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daaaf49c-defa-435e-bd57-7c5784992f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文档中创建索引\n",
    "from llama_index.core import VectorStoreIndex\n",
    "A_index = VectorStoreIndex.from_documents(A_docs)\n",
    "B_index = VectorStoreIndex.from_documents(B_docs)\n",
    "\n",
    "# 持久化索引（保存到本地）\n",
    "from llama_index.core import StorageContext\n",
    "A_index.storage_context.persist(persist_dir=\"./storage/A\")\n",
    "B_index.storage_context.persist(persist_dir=\"./storage/B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b66b46-c05a-4c46-a023-9a30ea05dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从本地读取索引\n",
    "from llama_index.core import load_index_from_storage\n",
    "try:\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=\"./storage/A\"\n",
    "    )\n",
    "    A_index = load_index_from_storage(storage_context)\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=\"./storage/B\"\n",
    "    )\n",
    "    B_index = load_index_from_storage(storage_context)\n",
    "\n",
    "    index_loaded = True\n",
    "except:\n",
    "    index_loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d0ab4d8-561a-48d1-847e-65a9838ecb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 创建查询引擎\n",
    "# A_engine = A_index.as_query_engine(similarity_top_k=3)\n",
    "# B_engine = B_index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "A_engine = A_index.as_query_engine(llm=model,\n",
    "                                  similarity_top_k=3)\n",
    "B_engine = B_index.as_query_engine(llm=model,\n",
    "                                  similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46981ef-1c8a-4e2c-8159-0bbf71c71f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置查询工具\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.tools import ToolMetadata\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=A_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"A_Finance\",\n",
    "            description=(\n",
    "                \"用于提供A公司的财务信息 \"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=B_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"B_Finance\",\n",
    "            description=(\n",
    "                \"用于提供B公司的财务信息 \"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06bd8f02-c8d7-4992-b7f9-99faa82e4fbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'context_window'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 创建ReAct Agent\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReActAgent\n\u001b[0;32m----> 3\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mReActAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_engine_tools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 让Agent完成任务\u001b[39;00m\n\u001b[1;32m      9\u001b[0m agent\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m比较一下两个公司的销售额\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/llama_index/core/agent/react/base.py:132\u001b[0m, in \u001b[0;36mReActAgent.from_tools\u001b[0;34m(cls, tools, tool_retriever, llm, chat_history, memory, memory_cls, max_iterations, react_chat_formatter, output_parser, callback_manager, verbose, context, handle_reasoning_failure_fn, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n\u001b[0;32m--> 132\u001b[0m memory \u001b[38;5;241m=\u001b[39m memory \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mmemory_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_history\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    136\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m [],\n\u001b[1;32m    137\u001b[0m     tool_retriever\u001b[38;5;241m=\u001b[39mtool_retriever,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     handle_reasoning_failure_fn\u001b[38;5;241m=\u001b[39mhandle_reasoning_failure_fn,\n\u001b[1;32m    147\u001b[0m )\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/llama_index/core/memory/chat_memory_buffer.py:63\u001b[0m, in \u001b[0;36mChatMemoryBuffer.from_defaults\u001b[0;34m(cls, chat_history, llm, chat_store, chat_store_key, token_limit, tokenizer_fn, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     context_window \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_window\u001b[49m\n\u001b[1;32m     64\u001b[0m     token_limit \u001b[38;5;241m=\u001b[39m token_limit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mint\u001b[39m(context_window \u001b[38;5;241m*\u001b[39m DEFAULT_TOKEN_LIMIT_RATIO)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m token_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'context_window'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 创建ReAct Agent\n",
    "from llama_index.core.agent import ReActAgent\n",
    "agent = ReActAgent.from_tools(query_engine_tools,\n",
    "                              llm=model,\n",
    "                              verbose=True)\n",
    "\n",
    "\n",
    "# 让Agent完成任务\n",
    "agent.chat(\"比较一下两个公司的销售额\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac8c0d-328c-4f12-a423-6eea59c42ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
